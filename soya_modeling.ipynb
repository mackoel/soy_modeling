{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "soya_modeling.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mackoel/soy_modeling/blob/master/soya_modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "uZo3etVD_Xnq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Soy blossom period prediction model"
      ]
    },
    {
      "metadata": {
        "id": "8uvy2UF6O_zR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ]
    },
    {
      "metadata": {
        "id": "W5rFAVaqARsQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Configure environment"
      ]
    },
    {
      "metadata": {
        "id": "4ji6Pv5z19yh",
        "colab_type": "code",
        "outputId": "25cf4d13-7325-4fd0-ae20-3831cddf5cc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install pydot"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pydot in /usr/local/lib/python3.6/dist-packages (1.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.6/dist-packages (from pydot) (2.3.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TOEIMPH508ai",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Show the machine details the environment is running on"
      ]
    },
    {
      "metadata": {
        "id": "uEjfrbGN1VAg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Show CPU info"
      ]
    },
    {
      "metadata": {
        "id": "slRC3KmWy9B0",
        "colab_type": "code",
        "outputId": "b1ba2d05-4460-4cc8-a7ef-21ba1bb2954e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 975
        }
      },
      "cell_type": "code",
      "source": [
        "!cat /proc/cpuinfo"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "processor\t: 0\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 63\n",
            "model name\t: Intel(R) Xeon(R) CPU @ 2.30GHz\n",
            "stepping\t: 0\n",
            "microcode\t: 0x1\n",
            "cpu MHz\t\t: 2300.000\n",
            "cache size\t: 46080 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 2\n",
            "core id\t\t: 0\n",
            "cpu cores\t: 1\n",
            "apicid\t\t: 0\n",
            "initial apicid\t: 0\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 13\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms xsaveopt arat arch_capabilities\n",
            "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf\n",
            "bogomips\t: 4600.00\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 46 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n",
            "processor\t: 1\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 63\n",
            "model name\t: Intel(R) Xeon(R) CPU @ 2.30GHz\n",
            "stepping\t: 0\n",
            "microcode\t: 0x1\n",
            "cpu MHz\t\t: 2300.000\n",
            "cache size\t: 46080 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 2\n",
            "core id\t\t: 0\n",
            "cpu cores\t: 1\n",
            "apicid\t\t: 1\n",
            "initial apicid\t: 1\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 13\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms xsaveopt arat arch_capabilities\n",
            "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf\n",
            "bogomips\t: 4600.00\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 46 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DCtwxw0i1YA7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Show RAM info"
      ]
    },
    {
      "metadata": {
        "id": "cRZfNgbA1Rmq",
        "colab_type": "code",
        "outputId": "7e54860b-cd88-458e-cbf3-ce320329467e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        }
      },
      "cell_type": "code",
      "source": [
        "!cat /proc/meminfo"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MemTotal:       13335212 kB\n",
            "MemFree:        11124124 kB\n",
            "MemAvailable:   12632440 kB\n",
            "Buffers:           63860 kB\n",
            "Cached:          1608512 kB\n",
            "SwapCached:            0 kB\n",
            "Active:           518372 kB\n",
            "Inactive:        1448044 kB\n",
            "Active(anon):     271608 kB\n",
            "Inactive(anon):      340 kB\n",
            "Active(file):     246764 kB\n",
            "Inactive(file):  1447704 kB\n",
            "Unevictable:           0 kB\n",
            "Mlocked:               0 kB\n",
            "SwapTotal:             0 kB\n",
            "SwapFree:              0 kB\n",
            "Dirty:               628 kB\n",
            "Writeback:             0 kB\n",
            "AnonPages:        294160 kB\n",
            "Mapped:           155892 kB\n",
            "Shmem:               868 kB\n",
            "Slab:             141544 kB\n",
            "SReclaimable:     111016 kB\n",
            "SUnreclaim:        30528 kB\n",
            "KernelStack:        3796 kB\n",
            "PageTables:         4788 kB\n",
            "NFS_Unstable:          0 kB\n",
            "Bounce:                0 kB\n",
            "WritebackTmp:          0 kB\n",
            "CommitLimit:     6667604 kB\n",
            "Committed_AS:    1806416 kB\n",
            "VmallocTotal:   34359738367 kB\n",
            "VmallocUsed:           0 kB\n",
            "VmallocChunk:          0 kB\n",
            "AnonHugePages:         0 kB\n",
            "ShmemHugePages:        0 kB\n",
            "ShmemPmdMapped:        0 kB\n",
            "HugePages_Total:       0\n",
            "HugePages_Free:        0\n",
            "HugePages_Rsvd:        0\n",
            "HugePages_Surp:        0\n",
            "Hugepagesize:       2048 kB\n",
            "DirectMap4k:       79860 kB\n",
            "DirectMap2M:     5163008 kB\n",
            "DirectMap1G:    10485760 kB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "98yFKWiy1mOo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Show GPU info"
      ]
    },
    {
      "metadata": {
        "id": "sCustmJB1pRd",
        "colab_type": "code",
        "outputId": "a08f0da3-f7ab-4c7f-ef6f-59f1caf64eff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        }
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[name: \"/device:CPU:0\"\n",
              " device_type: \"CPU\"\n",
              " memory_limit: 268435456\n",
              " locality {\n",
              " }\n",
              " incarnation: 4172840873117237056, name: \"/device:XLA_CPU:0\"\n",
              " device_type: \"XLA_CPU\"\n",
              " memory_limit: 17179869184\n",
              " locality {\n",
              " }\n",
              " incarnation: 13477617590842650785\n",
              " physical_device_desc: \"device: XLA_CPU device\", name: \"/device:XLA_GPU:0\"\n",
              " device_type: \"XLA_GPU\"\n",
              " memory_limit: 17179869184\n",
              " locality {\n",
              " }\n",
              " incarnation: 9122805177183770068\n",
              " physical_device_desc: \"device: XLA_GPU device\", name: \"/device:GPU:0\"\n",
              " device_type: \"GPU\"\n",
              " memory_limit: 11281553818\n",
              " locality {\n",
              "   bus_id: 1\n",
              "   links {\n",
              "   }\n",
              " }\n",
              " incarnation: 14306915272300075695\n",
              " physical_device_desc: \"device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "CABv94y8HL3Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Import modules"
      ]
    },
    {
      "metadata": {
        "id": "b7tG1GdlF4V5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from matplotlib import cm\n",
        "from matplotlib import gridspec\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import metrics\n",
        "import random\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import backend as K\n",
        "from sklearn.metrics import r2_score as r2_metric"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fCkdb0IvyopK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ca6e4142-6620-49aa-a215-a077b1d6453b"
      },
      "cell_type": "code",
      "source": [
        "from keras import backend as K\n",
        "\n",
        "def r2(y_true, y_pred):\n",
        "    SS_res =  K.sum(K.square( y_true-y_pred )) \n",
        "    SS_tot = K.sum(K.square( y_true - K.mean(y_true) ) ) \n",
        "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "nJCZh6DAjeXR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Set up the dataset"
      ]
    },
    {
      "metadata": {
        "id": "jLMzr6Q9HjOn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Download a dataset from shared Drive folder. "
      ]
    },
    {
      "metadata": {
        "id": "JCbXhbKiHppQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "#2. Get the file\n",
        "downloaded = drive.CreateFile({'id':'1sT_MlFRu6t8OKJDS_3Yy8SYOCBUx0vRa'})\n",
        "downloaded.GetContentFile('soy_data.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ore9JMXyjk_q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Import data set into pandas dataframe. The dataset contains a missing value on one of the key features the model will be training with, so we need to remove that explicitly"
      ]
    },
    {
      "metadata": {
        "id": "cMyVIEldRP9u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2023
        },
        "outputId": "e12072d8-5730-43c7-efa4-44eedd01ed72"
      },
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('soy_data.csv', index_col=[0, 1])\n",
        "df.dropna()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>t_min</th>\n",
              "      <th>t_max</th>\n",
              "      <th>dlen</th>\n",
              "      <th>day</th>\n",
              "      <th>state</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th>Unnamed: 1</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"30\" valign=\"top\">0</th>\n",
              "      <th>0</th>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.021642</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.043739</td>\n",
              "      <td>0.022222</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.066282</td>\n",
              "      <td>0.044444</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.089261</td>\n",
              "      <td>0.066667</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.112667</td>\n",
              "      <td>0.088889</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.04375</td>\n",
              "      <td>0.136488</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.00750</td>\n",
              "      <td>0.10625</td>\n",
              "      <td>0.160715</td>\n",
              "      <td>0.133333</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.02000</td>\n",
              "      <td>0.17750</td>\n",
              "      <td>0.185336</td>\n",
              "      <td>0.155556</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.06750</td>\n",
              "      <td>0.23375</td>\n",
              "      <td>0.210340</td>\n",
              "      <td>0.177778</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.06750</td>\n",
              "      <td>0.23375</td>\n",
              "      <td>0.235717</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.08125</td>\n",
              "      <td>0.23375</td>\n",
              "      <td>0.261454</td>\n",
              "      <td>0.222222</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.08750</td>\n",
              "      <td>0.23375</td>\n",
              "      <td>0.287540</td>\n",
              "      <td>0.244444</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.12625</td>\n",
              "      <td>0.23375</td>\n",
              "      <td>0.313963</td>\n",
              "      <td>0.266667</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.16750</td>\n",
              "      <td>0.23375</td>\n",
              "      <td>0.340710</td>\n",
              "      <td>0.288889</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.18000</td>\n",
              "      <td>0.25250</td>\n",
              "      <td>0.367770</td>\n",
              "      <td>0.311111</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.23125</td>\n",
              "      <td>0.26750</td>\n",
              "      <td>0.395129</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.26875</td>\n",
              "      <td>0.26750</td>\n",
              "      <td>0.422774</td>\n",
              "      <td>0.355556</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.30000</td>\n",
              "      <td>0.26750</td>\n",
              "      <td>0.450693</td>\n",
              "      <td>0.377778</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.32500</td>\n",
              "      <td>0.26750</td>\n",
              "      <td>0.478872</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.32500</td>\n",
              "      <td>0.27500</td>\n",
              "      <td>0.507297</td>\n",
              "      <td>0.422222</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.36625</td>\n",
              "      <td>0.27500</td>\n",
              "      <td>0.535956</td>\n",
              "      <td>0.444444</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.41750</td>\n",
              "      <td>0.31000</td>\n",
              "      <td>0.564833</td>\n",
              "      <td>0.466667</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.44625</td>\n",
              "      <td>0.33375</td>\n",
              "      <td>0.593915</td>\n",
              "      <td>0.488889</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.50875</td>\n",
              "      <td>0.33375</td>\n",
              "      <td>0.623188</td>\n",
              "      <td>0.511111</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.53875</td>\n",
              "      <td>0.33375</td>\n",
              "      <td>0.652637</td>\n",
              "      <td>0.533333</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.53875</td>\n",
              "      <td>0.33375</td>\n",
              "      <td>0.682248</td>\n",
              "      <td>0.555556</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.54500</td>\n",
              "      <td>0.33625</td>\n",
              "      <td>0.712006</td>\n",
              "      <td>0.577778</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.54500</td>\n",
              "      <td>0.39000</td>\n",
              "      <td>0.741897</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.56750</td>\n",
              "      <td>0.46125</td>\n",
              "      <td>0.771905</td>\n",
              "      <td>0.622222</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.64500</td>\n",
              "      <td>0.50500</td>\n",
              "      <td>0.802016</td>\n",
              "      <td>0.644444</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"30\" valign=\"top\">152</th>\n",
              "      <th>16</th>\n",
              "      <td>0.09875</td>\n",
              "      <td>0.00625</td>\n",
              "      <td>1.109319</td>\n",
              "      <td>0.355556</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.11875</td>\n",
              "      <td>0.04750</td>\n",
              "      <td>1.176748</td>\n",
              "      <td>0.377778</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.11875</td>\n",
              "      <td>0.04750</td>\n",
              "      <td>1.244203</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.11875</td>\n",
              "      <td>0.04750</td>\n",
              "      <td>1.311650</td>\n",
              "      <td>0.422222</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.11875</td>\n",
              "      <td>0.04750</td>\n",
              "      <td>1.379053</td>\n",
              "      <td>0.444444</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.16125</td>\n",
              "      <td>0.07750</td>\n",
              "      <td>1.446377</td>\n",
              "      <td>0.466667</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.21125</td>\n",
              "      <td>0.07750</td>\n",
              "      <td>1.513586</td>\n",
              "      <td>0.488889</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.23375</td>\n",
              "      <td>0.07750</td>\n",
              "      <td>1.580647</td>\n",
              "      <td>0.511111</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.24875</td>\n",
              "      <td>0.07750</td>\n",
              "      <td>1.647525</td>\n",
              "      <td>0.533333</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.27625</td>\n",
              "      <td>0.09875</td>\n",
              "      <td>1.714185</td>\n",
              "      <td>0.555556</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.29500</td>\n",
              "      <td>0.16000</td>\n",
              "      <td>1.780594</td>\n",
              "      <td>0.577778</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.35125</td>\n",
              "      <td>0.20750</td>\n",
              "      <td>1.846719</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.40625</td>\n",
              "      <td>0.20750</td>\n",
              "      <td>1.912526</td>\n",
              "      <td>0.622222</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.42750</td>\n",
              "      <td>0.20750</td>\n",
              "      <td>1.977984</td>\n",
              "      <td>0.644444</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.44500</td>\n",
              "      <td>0.24750</td>\n",
              "      <td>2.043060</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0.48250</td>\n",
              "      <td>0.31750</td>\n",
              "      <td>2.107724</td>\n",
              "      <td>0.688889</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.55125</td>\n",
              "      <td>0.40625</td>\n",
              "      <td>2.171945</td>\n",
              "      <td>0.711111</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0.66375</td>\n",
              "      <td>0.51125</td>\n",
              "      <td>2.235693</td>\n",
              "      <td>0.733333</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>0.77500</td>\n",
              "      <td>0.62375</td>\n",
              "      <td>2.298940</td>\n",
              "      <td>0.755556</td>\n",
              "      <td>0.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0.86125</td>\n",
              "      <td>0.66750</td>\n",
              "      <td>2.361657</td>\n",
              "      <td>0.777778</td>\n",
              "      <td>0.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>0.94875</td>\n",
              "      <td>0.70500</td>\n",
              "      <td>2.423816</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>1.03125</td>\n",
              "      <td>0.77875</td>\n",
              "      <td>2.485391</td>\n",
              "      <td>0.822222</td>\n",
              "      <td>0.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>1.12625</td>\n",
              "      <td>0.88375</td>\n",
              "      <td>2.546355</td>\n",
              "      <td>0.844444</td>\n",
              "      <td>0.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>1.23125</td>\n",
              "      <td>0.99500</td>\n",
              "      <td>2.606684</td>\n",
              "      <td>0.866667</td>\n",
              "      <td>0.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>1.38125</td>\n",
              "      <td>1.11375</td>\n",
              "      <td>2.666353</td>\n",
              "      <td>0.888889</td>\n",
              "      <td>0.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>1.51250</td>\n",
              "      <td>1.24000</td>\n",
              "      <td>2.725338</td>\n",
              "      <td>0.911111</td>\n",
              "      <td>0.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>1.66250</td>\n",
              "      <td>1.34500</td>\n",
              "      <td>2.783617</td>\n",
              "      <td>0.933333</td>\n",
              "      <td>0.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>1.78000</td>\n",
              "      <td>1.36750</td>\n",
              "      <td>2.841168</td>\n",
              "      <td>0.955556</td>\n",
              "      <td>0.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>1.88250</td>\n",
              "      <td>1.40500</td>\n",
              "      <td>2.897969</td>\n",
              "      <td>0.977778</td>\n",
              "      <td>0.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>1.99000</td>\n",
              "      <td>1.46250</td>\n",
              "      <td>2.954001</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.9</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7038 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                  t_min    t_max      dlen       day  state\n",
              "    Unnamed: 1                                             \n",
              "0   0           0.00000  0.00000  0.021642  0.000000    0.1\n",
              "    1           0.00000  0.00000  0.043739  0.022222    0.1\n",
              "    2           0.00000  0.00000  0.066282  0.044444    0.1\n",
              "    3           0.00000  0.00000  0.089261  0.066667    0.1\n",
              "    4           0.00000  0.00000  0.112667  0.088889    0.1\n",
              "    5           0.00000  0.04375  0.136488  0.111111    0.1\n",
              "    6           0.00750  0.10625  0.160715  0.133333    0.1\n",
              "    7           0.02000  0.17750  0.185336  0.155556    0.1\n",
              "    8           0.06750  0.23375  0.210340  0.177778    0.1\n",
              "    9           0.06750  0.23375  0.235717  0.200000    0.1\n",
              "    10          0.08125  0.23375  0.261454  0.222222    0.1\n",
              "    11          0.08750  0.23375  0.287540  0.244444    0.1\n",
              "    12          0.12625  0.23375  0.313963  0.266667    0.1\n",
              "    13          0.16750  0.23375  0.340710  0.288889    0.1\n",
              "    14          0.18000  0.25250  0.367770  0.311111    0.1\n",
              "    15          0.23125  0.26750  0.395129  0.333333    0.1\n",
              "    16          0.26875  0.26750  0.422774  0.355556    0.1\n",
              "    17          0.30000  0.26750  0.450693  0.377778    0.1\n",
              "    18          0.32500  0.26750  0.478872  0.400000    0.1\n",
              "    19          0.32500  0.27500  0.507297  0.422222    0.1\n",
              "    20          0.36625  0.27500  0.535956  0.444444    0.1\n",
              "    21          0.41750  0.31000  0.564833  0.466667    0.1\n",
              "    22          0.44625  0.33375  0.593915  0.488889    0.1\n",
              "    23          0.50875  0.33375  0.623188  0.511111    0.1\n",
              "    24          0.53875  0.33375  0.652637  0.533333    0.1\n",
              "    25          0.53875  0.33375  0.682248  0.555556    0.1\n",
              "    26          0.54500  0.33625  0.712006  0.577778    0.1\n",
              "    27          0.54500  0.39000  0.741897  0.600000    0.1\n",
              "    28          0.56750  0.46125  0.771905  0.622222    0.1\n",
              "    29          0.64500  0.50500  0.802016  0.644444    0.1\n",
              "...                 ...      ...       ...       ...    ...\n",
              "152 16          0.09875  0.00625  1.109319  0.355556    0.1\n",
              "    17          0.11875  0.04750  1.176748  0.377778    0.1\n",
              "    18          0.11875  0.04750  1.244203  0.400000    0.1\n",
              "    19          0.11875  0.04750  1.311650  0.422222    0.1\n",
              "    20          0.11875  0.04750  1.379053  0.444444    0.1\n",
              "    21          0.16125  0.07750  1.446377  0.466667    0.1\n",
              "    22          0.21125  0.07750  1.513586  0.488889    0.1\n",
              "    23          0.23375  0.07750  1.580647  0.511111    0.1\n",
              "    24          0.24875  0.07750  1.647525  0.533333    0.1\n",
              "    25          0.27625  0.09875  1.714185  0.555556    0.1\n",
              "    26          0.29500  0.16000  1.780594  0.577778    0.1\n",
              "    27          0.35125  0.20750  1.846719  0.600000    0.1\n",
              "    28          0.40625  0.20750  1.912526  0.622222    0.1\n",
              "    29          0.42750  0.20750  1.977984  0.644444    0.1\n",
              "    30          0.44500  0.24750  2.043060  0.666667    0.1\n",
              "    31          0.48250  0.31750  2.107724  0.688889    0.1\n",
              "    32          0.55125  0.40625  2.171945  0.711111    0.1\n",
              "    33          0.66375  0.51125  2.235693  0.733333    0.2\n",
              "    34          0.77500  0.62375  2.298940  0.755556    0.3\n",
              "    35          0.86125  0.66750  2.361657  0.777778    0.4\n",
              "    36          0.94875  0.70500  2.423816  0.800000    0.5\n",
              "    37          1.03125  0.77875  2.485391  0.822222    0.6\n",
              "    38          1.12625  0.88375  2.546355  0.844444    0.7\n",
              "    39          1.23125  0.99500  2.606684  0.866667    0.8\n",
              "    40          1.38125  1.11375  2.666353  0.888889    0.9\n",
              "    41          1.51250  1.24000  2.725338  0.911111    0.9\n",
              "    42          1.66250  1.34500  2.783617  0.933333    0.9\n",
              "    43          1.78000  1.36750  2.841168  0.955556    0.9\n",
              "    44          1.88250  1.40500  2.897969  0.977778    0.9\n",
              "    45          1.99000  1.46250  2.954001  1.000000    0.9\n",
              "\n",
              "[7038 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "ihMUMlDPp5j8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The data in now loaded, it uses format that will be described somewhere here some day. "
      ]
    },
    {
      "metadata": {
        "id": "TRck9wib-rG8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Feature extraction"
      ]
    },
    {
      "metadata": {
        "id": "EFakYqHpTYb5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Shuffle the data. As dataset uses multiindex that is a little tricky, we need to shuffle only the certain level, but the days inside a single experiment shall be in natural order."
      ]
    },
    {
      "metadata": {
        "id": "nv7EdGARsKQG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "shuffled_indexes = [i for i in range(len(df.index.levels[0]))]\n",
        "random.shuffle(shuffled_indexes)\n",
        "new_indexes = sorted(df.index, key=lambda x: shuffled_indexes.index(x[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qMKWEaBhTVDA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df = df.reindex(new_indexes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pt8STFIhrFY7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2023
        },
        "outputId": "8981204d-8b39-4f59-eebd-27dcd7c7f1b8"
      },
      "cell_type": "code",
      "source": [
        "df"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>t_min</th>\n",
              "      <th>t_max</th>\n",
              "      <th>dlen</th>\n",
              "      <th>day</th>\n",
              "      <th>state</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th>Unnamed: 1</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"30\" valign=\"top\">41</th>\n",
              "      <th>0</th>\n",
              "      <td>0.03500</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.063988</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.05875</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.128435</td>\n",
              "      <td>0.022222</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.05875</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.193309</td>\n",
              "      <td>0.044444</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.05875</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.258580</td>\n",
              "      <td>0.066667</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.05875</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.324216</td>\n",
              "      <td>0.088889</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.05875</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.390186</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.05875</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.456457</td>\n",
              "      <td>0.133333</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.05875</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.522996</td>\n",
              "      <td>0.155556</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.05875</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.589769</td>\n",
              "      <td>0.177778</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.05875</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.656743</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.08375</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.723882</td>\n",
              "      <td>0.222222</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.10000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.791153</td>\n",
              "      <td>0.244444</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.13375</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.858521</td>\n",
              "      <td>0.266667</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.15750</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.925950</td>\n",
              "      <td>0.288889</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.16750</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.993406</td>\n",
              "      <td>0.311111</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.21000</td>\n",
              "      <td>0.00500</td>\n",
              "      <td>1.060852</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.25625</td>\n",
              "      <td>0.01125</td>\n",
              "      <td>1.128255</td>\n",
              "      <td>0.355556</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.26750</td>\n",
              "      <td>0.01125</td>\n",
              "      <td>1.195579</td>\n",
              "      <td>0.377778</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.26750</td>\n",
              "      <td>0.01125</td>\n",
              "      <td>1.262789</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.26750</td>\n",
              "      <td>0.01125</td>\n",
              "      <td>1.329849</td>\n",
              "      <td>0.422222</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.26750</td>\n",
              "      <td>0.01375</td>\n",
              "      <td>1.396727</td>\n",
              "      <td>0.444444</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.27625</td>\n",
              "      <td>0.04000</td>\n",
              "      <td>1.463387</td>\n",
              "      <td>0.466667</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.33250</td>\n",
              "      <td>0.04250</td>\n",
              "      <td>1.529796</td>\n",
              "      <td>0.488889</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.38625</td>\n",
              "      <td>0.09625</td>\n",
              "      <td>1.595921</td>\n",
              "      <td>0.511111</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.45875</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>1.661728</td>\n",
              "      <td>0.533333</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.50625</td>\n",
              "      <td>0.13875</td>\n",
              "      <td>1.727186</td>\n",
              "      <td>0.555556</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.55750</td>\n",
              "      <td>0.14875</td>\n",
              "      <td>1.792262</td>\n",
              "      <td>0.577778</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.62000</td>\n",
              "      <td>0.21125</td>\n",
              "      <td>1.856926</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.70750</td>\n",
              "      <td>0.26250</td>\n",
              "      <td>1.921147</td>\n",
              "      <td>0.622222</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.75625</td>\n",
              "      <td>0.30750</td>\n",
              "      <td>1.984896</td>\n",
              "      <td>0.644444</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"30\" valign=\"top\">126</th>\n",
              "      <th>16</th>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.07625</td>\n",
              "      <td>1.081821</td>\n",
              "      <td>0.355556</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.21125</td>\n",
              "      <td>0.07625</td>\n",
              "      <td>1.148795</td>\n",
              "      <td>0.377778</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.23000</td>\n",
              "      <td>0.07625</td>\n",
              "      <td>1.215934</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.23000</td>\n",
              "      <td>0.07625</td>\n",
              "      <td>1.283205</td>\n",
              "      <td>0.422222</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.23000</td>\n",
              "      <td>0.07625</td>\n",
              "      <td>1.350573</td>\n",
              "      <td>0.444444</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.23000</td>\n",
              "      <td>0.07625</td>\n",
              "      <td>1.418002</td>\n",
              "      <td>0.466667</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.23000</td>\n",
              "      <td>0.07625</td>\n",
              "      <td>1.485458</td>\n",
              "      <td>0.488889</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.23000</td>\n",
              "      <td>0.07625</td>\n",
              "      <td>1.552904</td>\n",
              "      <td>0.511111</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.23000</td>\n",
              "      <td>0.07625</td>\n",
              "      <td>1.620307</td>\n",
              "      <td>0.533333</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.23500</td>\n",
              "      <td>0.07625</td>\n",
              "      <td>1.687631</td>\n",
              "      <td>0.555556</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.23500</td>\n",
              "      <td>0.07625</td>\n",
              "      <td>1.754841</td>\n",
              "      <td>0.577778</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.23875</td>\n",
              "      <td>0.07625</td>\n",
              "      <td>1.821901</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.24875</td>\n",
              "      <td>0.07625</td>\n",
              "      <td>1.888779</td>\n",
              "      <td>0.622222</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.26125</td>\n",
              "      <td>0.07625</td>\n",
              "      <td>1.955439</td>\n",
              "      <td>0.644444</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.27500</td>\n",
              "      <td>0.07625</td>\n",
              "      <td>2.021849</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0.28500</td>\n",
              "      <td>0.07625</td>\n",
              "      <td>2.087973</td>\n",
              "      <td>0.688889</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.28750</td>\n",
              "      <td>0.08250</td>\n",
              "      <td>2.153780</td>\n",
              "      <td>0.711111</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0.31625</td>\n",
              "      <td>0.10750</td>\n",
              "      <td>2.219238</td>\n",
              "      <td>0.733333</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>0.36750</td>\n",
              "      <td>0.15500</td>\n",
              "      <td>2.284315</td>\n",
              "      <td>0.755556</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0.41750</td>\n",
              "      <td>0.15500</td>\n",
              "      <td>2.348978</td>\n",
              "      <td>0.777778</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>0.46375</td>\n",
              "      <td>0.17250</td>\n",
              "      <td>2.413199</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0.51125</td>\n",
              "      <td>0.17250</td>\n",
              "      <td>2.476948</td>\n",
              "      <td>0.822222</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>0.55500</td>\n",
              "      <td>0.17250</td>\n",
              "      <td>2.540194</td>\n",
              "      <td>0.844444</td>\n",
              "      <td>0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>0.60750</td>\n",
              "      <td>0.17500</td>\n",
              "      <td>2.602911</td>\n",
              "      <td>0.866667</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>0.66250</td>\n",
              "      <td>0.17500</td>\n",
              "      <td>2.665070</td>\n",
              "      <td>0.888889</td>\n",
              "      <td>0.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>0.68750</td>\n",
              "      <td>0.18750</td>\n",
              "      <td>2.726645</td>\n",
              "      <td>0.911111</td>\n",
              "      <td>0.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>0.74125</td>\n",
              "      <td>0.18750</td>\n",
              "      <td>2.787609</td>\n",
              "      <td>0.933333</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>0.76375</td>\n",
              "      <td>0.18750</td>\n",
              "      <td>2.847938</td>\n",
              "      <td>0.955556</td>\n",
              "      <td>0.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>0.82125</td>\n",
              "      <td>0.18750</td>\n",
              "      <td>2.907607</td>\n",
              "      <td>0.977778</td>\n",
              "      <td>0.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>0.86250</td>\n",
              "      <td>0.18750</td>\n",
              "      <td>2.966592</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.8</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7038 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                  t_min    t_max      dlen       day  state\n",
              "    Unnamed: 1                                             \n",
              "41  0           0.03500  0.00000  0.063988  0.000000    0.1\n",
              "    1           0.05875  0.00000  0.128435  0.022222    0.1\n",
              "    2           0.05875  0.00000  0.193309  0.044444    0.1\n",
              "    3           0.05875  0.00000  0.258580  0.066667    0.1\n",
              "    4           0.05875  0.00000  0.324216  0.088889    0.1\n",
              "    5           0.05875  0.00000  0.390186  0.111111    0.1\n",
              "    6           0.05875  0.00000  0.456457  0.133333    0.1\n",
              "    7           0.05875  0.00000  0.522996  0.155556    0.1\n",
              "    8           0.05875  0.00000  0.589769  0.177778    0.1\n",
              "    9           0.05875  0.00000  0.656743  0.200000    0.1\n",
              "    10          0.08375  0.00000  0.723882  0.222222    0.1\n",
              "    11          0.10000  0.00000  0.791153  0.244444    0.1\n",
              "    12          0.13375  0.00000  0.858521  0.266667    0.1\n",
              "    13          0.15750  0.00000  0.925950  0.288889    0.1\n",
              "    14          0.16750  0.00000  0.993406  0.311111    0.1\n",
              "    15          0.21000  0.00500  1.060852  0.333333    0.1\n",
              "    16          0.25625  0.01125  1.128255  0.355556    0.1\n",
              "    17          0.26750  0.01125  1.195579  0.377778    0.1\n",
              "    18          0.26750  0.01125  1.262789  0.400000    0.1\n",
              "    19          0.26750  0.01125  1.329849  0.422222    0.1\n",
              "    20          0.26750  0.01375  1.396727  0.444444    0.1\n",
              "    21          0.27625  0.04000  1.463387  0.466667    0.1\n",
              "    22          0.33250  0.04250  1.529796  0.488889    0.1\n",
              "    23          0.38625  0.09625  1.595921  0.511111    0.1\n",
              "    24          0.45875  0.13875  1.661728  0.533333    0.1\n",
              "    25          0.50625  0.13875  1.727186  0.555556    0.1\n",
              "    26          0.55750  0.14875  1.792262  0.577778    0.1\n",
              "    27          0.62000  0.21125  1.856926  0.600000    0.1\n",
              "    28          0.70750  0.26250  1.921147  0.622222    0.1\n",
              "    29          0.75625  0.30750  1.984896  0.644444    0.1\n",
              "...                 ...      ...       ...       ...    ...\n",
              "126 16          0.20500  0.07625  1.081821  0.355556    0.1\n",
              "    17          0.21125  0.07625  1.148795  0.377778    0.1\n",
              "    18          0.23000  0.07625  1.215934  0.400000    0.1\n",
              "    19          0.23000  0.07625  1.283205  0.422222    0.1\n",
              "    20          0.23000  0.07625  1.350573  0.444444    0.1\n",
              "    21          0.23000  0.07625  1.418002  0.466667    0.1\n",
              "    22          0.23000  0.07625  1.485458  0.488889    0.1\n",
              "    23          0.23000  0.07625  1.552904  0.511111    0.1\n",
              "    24          0.23000  0.07625  1.620307  0.533333    0.1\n",
              "    25          0.23500  0.07625  1.687631  0.555556    0.1\n",
              "    26          0.23500  0.07625  1.754841  0.577778    0.1\n",
              "    27          0.23875  0.07625  1.821901  0.600000    0.1\n",
              "    28          0.24875  0.07625  1.888779  0.622222    0.1\n",
              "    29          0.26125  0.07625  1.955439  0.644444    0.1\n",
              "    30          0.27500  0.07625  2.021849  0.666667    0.1\n",
              "    31          0.28500  0.07625  2.087973  0.688889    0.1\n",
              "    32          0.28750  0.08250  2.153780  0.711111    0.1\n",
              "    33          0.31625  0.10750  2.219238  0.733333    0.1\n",
              "    34          0.36750  0.15500  2.284315  0.755556    0.1\n",
              "    35          0.41750  0.15500  2.348978  0.777778    0.1\n",
              "    36          0.46375  0.17250  2.413199  0.800000    0.1\n",
              "    37          0.51125  0.17250  2.476948  0.822222    0.1\n",
              "    38          0.55500  0.17250  2.540194  0.844444    0.1\n",
              "    39          0.60750  0.17500  2.602911  0.866667    0.2\n",
              "    40          0.66250  0.17500  2.665070  0.888889    0.3\n",
              "    41          0.68750  0.18750  2.726645  0.911111    0.4\n",
              "    42          0.74125  0.18750  2.787609  0.933333    0.5\n",
              "    43          0.76375  0.18750  2.847938  0.955556    0.6\n",
              "    44          0.82125  0.18750  2.907607  0.977778    0.7\n",
              "    45          0.86250  0.18750  2.966592  1.000000    0.8\n",
              "\n",
              "[7038 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "metadata": {
        "id": "DMi7GUsSTjD-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Select features and target parameter. As the data is shuffled at this point we no longer need multi-index, and selecting values will truncate that information automatically."
      ]
    },
    {
      "metadata": {
        "id": "dlpnB55syxkr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "features = ['t_min', 't_max', 'dlen', 'day']\n",
        "data = df[features].values\n",
        "target = df['state'].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AShelcevMkUJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "data is numpy matrix with only the needed features for the model and target is numpy vector with output data"
      ]
    },
    {
      "metadata": {
        "id": "gatua_0HNPsw",
        "colab_type": "code",
        "outputId": "57f70bc6-e921-49e6-c9ea-bb65b1773336",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "cell_type": "code",
      "source": [
        "#@title Select number of rows to display { run: \"auto\", form-width: \"30%\" }\n",
        "num_entries = 3 #@param {type:\"slider\", min:0, max:5, step:1}\n",
        "print(\"Data:  \", data.shape, \"Showing only\", num_entries, \"\\n\", data[0:num_entries], \"\\n\")\n",
        "print(\"Target:\", target.shape, \"Showing only\", num_entries, \"\\n\", target[0:num_entries], \"\\n\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data:   (7038, 4) Showing only 3 \n",
            " [[0.035      0.         0.06398836 0.        ]\n",
            " [0.05875    0.         0.12843459 0.02222222]\n",
            " [0.05875    0.         0.19330857 0.04444444]] \n",
            "\n",
            "Target: (7038,) Showing only 3 \n",
            " [0.1 0.1 0.1] \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZTmbe365Qjz8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Split the data for training and testing"
      ]
    },
    {
      "metadata": {
        "id": "gnJFOFwpQjV9",
        "colab_type": "code",
        "outputId": "41f524bd-2c33-48e9-b8bb-f7ff426be0d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "#@title Set the desired partition of test and train samples { run: \"auto\", form-width: \"30%\", display-mode: \"both\" }\n",
        "border = 6100 #@param {type:\"slider\", min:4000, max:6100, step:30}\n",
        "train_input, train_output = data[:border], target[:border]\n",
        "test_input, test_output = data[border:], target[border:]\n",
        "\n",
        "print(\"Train set contains\", len(train_input))\n",
        "print(\"Test set contains\", len(test_input))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train set contains 6100\n",
            "Test set contains 938\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KdueMwofOzg4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##  Model"
      ]
    },
    {
      "metadata": {
        "id": "h2TevvaLPGZW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We will use a simple model with input layer of 4 neurons, a single hidden layer with 20 neurons and 1 output neuron"
      ]
    },
    {
      "metadata": {
        "id": "e2PvleuRPY__",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.enable_eager_execution()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7KmKSmm-YL5U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Keras model definition"
      ]
    },
    {
      "metadata": {
        "id": "TCiZPeGAPuhZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = keras.Sequential()\n",
        "model.add(keras.layers.Dense(20, input_dim=4, activation=tf.nn.sigmoid))\n",
        "model.add(keras.layers.Dense(1, activation=tf.nn.sigmoid))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TeRWlA-oYU2b",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Keras model compile"
      ]
    },
    {
      "metadata": {
        "id": "SPIy9SW8PyFC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile(loss='mse',\n",
        "              optimizer=tf.train.GradientDescentOptimizer(learning_rate=10.),\n",
        "              metrics=['mse', 'mae', r2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nzQE8hoXWRNb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Model summary shows the architecture of the keras model"
      ]
    },
    {
      "metadata": {
        "id": "2g_Nn5dwWL2U",
        "colab_type": "code",
        "outputId": "646cc2d2-5e8a-4117-d7ff-9202f9c8ceda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "cell_type": "code",
      "source": [
        "print(model.summary())"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 20)                100       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 21        \n",
            "=================================================================\n",
            "Total params: 121\n",
            "Trainable params: 121\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1yF62adxYEPh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Results"
      ]
    },
    {
      "metadata": {
        "id": "sX3hnRLUPz_b",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "7d50cfe5-4c07-45a4-e4b2-a9dab19c592d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6063
        }
      },
      "cell_type": "code",
      "source": [
        "#@title Hyperparameters { run: \"auto\", form-width: \"30%\" }\n",
        "epochs = 100 #@param {type:\"integer\"}\n",
        "history = model.fit(\n",
        "    train_input, train_output,\n",
        "    epochs=epochs,\n",
        "    verbose=1,\n",
        "    batch_size=35,\n",
        "    validation_split=0.25)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0229 - mean_squared_error: 0.0229 - mean_absolute_error: 0.1206 - r2: -628310.9117 - val_loss: 0.0235 - val_mean_squared_error: 0.0235 - val_mean_absolute_error: 0.1247 - val_r2: -871740.6424\n",
            "Epoch 28/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0240 - mean_squared_error: 0.0240 - mean_absolute_error: 0.1217 - r2: -692908.4035 - val_loss: 0.0228 - val_mean_squared_error: 0.0228 - val_mean_absolute_error: 0.1232 - val_r2: -906014.1604\n",
            "Epoch 29/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0240 - mean_squared_error: 0.0240 - mean_absolute_error: 0.1215 - r2: -842898.2172 - val_loss: 0.0263 - val_mean_squared_error: 0.0263 - val_mean_absolute_error: 0.1237 - val_r2: -2109619.3164\n",
            "Epoch 30/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0221 - mean_squared_error: 0.0221 - mean_absolute_error: 0.1178 - r2: -605237.2099 - val_loss: 0.0209 - val_mean_squared_error: 0.0209 - val_mean_absolute_error: 0.1174 - val_r2: -1543783.0810\n",
            "Epoch 31/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0237 - mean_squared_error: 0.0237 - mean_absolute_error: 0.1227 - r2: -728963.2270 - val_loss: 0.0253 - val_mean_squared_error: 0.0253 - val_mean_absolute_error: 0.1219 - val_r2: -2047804.9637\n",
            "Epoch 32/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0237 - mean_squared_error: 0.0237 - mean_absolute_error: 0.1207 - r2: -865862.9517 - val_loss: 0.0211 - val_mean_squared_error: 0.0211 - val_mean_absolute_error: 0.1221 - val_r2: -1020314.5543\n",
            "Epoch 33/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0230 - mean_squared_error: 0.0230 - mean_absolute_error: 0.1217 - r2: -653213.7197 - val_loss: 0.0412 - val_mean_squared_error: 0.0412 - val_mean_absolute_error: 0.1416 - val_r2: -3245094.5015\n",
            "Epoch 34/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0235 - mean_squared_error: 0.0235 - mean_absolute_error: 0.1206 - r2: -824457.4653 - val_loss: 0.0243 - val_mean_squared_error: 0.0243 - val_mean_absolute_error: 0.1287 - val_r2: -884449.4986\n",
            "Epoch 35/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0231 - mean_squared_error: 0.0231 - mean_absolute_error: 0.1200 - r2: -750127.1943 - val_loss: 0.0214 - val_mean_squared_error: 0.0214 - val_mean_absolute_error: 0.1178 - val_r2: -1646037.3989\n",
            "Epoch 36/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0240 - mean_squared_error: 0.0240 - mean_absolute_error: 0.1211 - r2: -591765.3596 - val_loss: 0.0239 - val_mean_squared_error: 0.0239 - val_mean_absolute_error: 0.1280 - val_r2: -905324.2151\n",
            "Epoch 37/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0223 - mean_squared_error: 0.0223 - mean_absolute_error: 0.1189 - r2: -918965.9580 - val_loss: 0.0239 - val_mean_squared_error: 0.0239 - val_mean_absolute_error: 0.1195 - val_r2: -1861481.8670\n",
            "Epoch 38/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0236 - mean_squared_error: 0.0236 - mean_absolute_error: 0.1212 - r2: -729657.8826 - val_loss: 0.0205 - val_mean_squared_error: 0.0205 - val_mean_absolute_error: 0.1169 - val_r2: -1453442.5018\n",
            "Epoch 39/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0235 - mean_squared_error: 0.0235 - mean_absolute_error: 0.1218 - r2: -770199.5387 - val_loss: 0.0207 - val_mean_squared_error: 0.0207 - val_mean_absolute_error: 0.1172 - val_r2: -1504827.6552\n",
            "Epoch 40/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0224 - mean_squared_error: 0.0224 - mean_absolute_error: 0.1190 - r2: -697188.5976 - val_loss: 0.0378 - val_mean_squared_error: 0.0378 - val_mean_absolute_error: 0.1368 - val_r2: -3011877.9092\n",
            "Epoch 41/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0221 - mean_squared_error: 0.0221 - mean_absolute_error: 0.1177 - r2: -585387.0713 - val_loss: 0.0202 - val_mean_squared_error: 0.0202 - val_mean_absolute_error: 0.1195 - val_r2: -1122998.4957\n",
            "Epoch 42/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0232 - mean_squared_error: 0.0232 - mean_absolute_error: 0.1205 - r2: -595574.0476 - val_loss: 0.0245 - val_mean_squared_error: 0.0245 - val_mean_absolute_error: 0.1202 - val_r2: -1965498.2651\n",
            "Epoch 43/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0237 - mean_squared_error: 0.0237 - mean_absolute_error: 0.1198 - r2: -647260.1179 - val_loss: 0.0217 - val_mean_squared_error: 0.0217 - val_mean_absolute_error: 0.1244 - val_r2: -1082852.4967\n",
            "Epoch 44/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0238 - mean_squared_error: 0.0238 - mean_absolute_error: 0.1217 - r2: -791427.4570 - val_loss: 0.0202 - val_mean_squared_error: 0.0202 - val_mean_absolute_error: 0.1189 - val_r2: -1145052.8717\n",
            "Epoch 45/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0227 - mean_squared_error: 0.0227 - mean_absolute_error: 0.1180 - r2: -888672.8468 - val_loss: 0.0222 - val_mean_squared_error: 0.0222 - val_mean_absolute_error: 0.1227 - val_r2: -929358.8583\n",
            "Epoch 46/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0231 - mean_squared_error: 0.0231 - mean_absolute_error: 0.1189 - r2: -595090.2605 - val_loss: 0.0223 - val_mean_squared_error: 0.0223 - val_mean_absolute_error: 0.1195 - val_r2: -1763099.4267\n",
            "Epoch 47/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0226 - mean_squared_error: 0.0226 - mean_absolute_error: 0.1194 - r2: -568491.8108 - val_loss: 0.0208 - val_mean_squared_error: 0.0208 - val_mean_absolute_error: 0.1188 - val_r2: -1534407.1077\n",
            "Epoch 48/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0227 - mean_squared_error: 0.0227 - mean_absolute_error: 0.1204 - r2: -708420.4717 - val_loss: 0.0209 - val_mean_squared_error: 0.0209 - val_mean_absolute_error: 0.1174 - val_r2: -1595021.8153\n",
            "Epoch 49/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0209 - mean_squared_error: 0.0209 - mean_absolute_error: 0.1172 - r2: -516952.0432 - val_loss: 0.0269 - val_mean_squared_error: 0.0269 - val_mean_absolute_error: 0.1227 - val_r2: -2188340.2494\n",
            "Epoch 50/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0233 - mean_squared_error: 0.0233 - mean_absolute_error: 0.1208 - r2: -650989.3487 - val_loss: 0.0306 - val_mean_squared_error: 0.0306 - val_mean_absolute_error: 0.1279 - val_r2: -2464130.5842\n",
            "Epoch 51/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0242 - mean_squared_error: 0.0242 - mean_absolute_error: 0.1234 - r2: -857009.2042 - val_loss: 0.0215 - val_mean_squared_error: 0.0215 - val_mean_absolute_error: 0.1217 - val_r2: -999546.0715\n",
            "Epoch 52/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0220 - mean_squared_error: 0.0220 - mean_absolute_error: 0.1173 - r2: -718362.6589 - val_loss: 0.0206 - val_mean_squared_error: 0.0206 - val_mean_absolute_error: 0.1168 - val_r2: -1499020.7995\n",
            "Epoch 53/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0219 - mean_squared_error: 0.0219 - mean_absolute_error: 0.1171 - r2: -927476.4685 - val_loss: 0.0205 - val_mean_squared_error: 0.0205 - val_mean_absolute_error: 0.1209 - val_r2: -1325876.9904\n",
            "Epoch 54/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0224 - mean_squared_error: 0.0224 - mean_absolute_error: 0.1193 - r2: -929286.4775 - val_loss: 0.0272 - val_mean_squared_error: 0.0272 - val_mean_absolute_error: 0.1236 - val_r2: -2257964.2707\n",
            "Epoch 55/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0228 - mean_squared_error: 0.0228 - mean_absolute_error: 0.1198 - r2: -554693.4771 - val_loss: 0.0276 - val_mean_squared_error: 0.0276 - val_mean_absolute_error: 0.1319 - val_r2: -774031.1750\n",
            "Epoch 56/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0211 - mean_squared_error: 0.0211 - mean_absolute_error: 0.1168 - r2: -605400.3246 - val_loss: 0.0212 - val_mean_squared_error: 0.0212 - val_mean_absolute_error: 0.1215 - val_r2: -1028202.5024\n",
            "Epoch 57/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0232 - mean_squared_error: 0.0232 - mean_absolute_error: 0.1207 - r2: -698787.7186 - val_loss: 0.0215 - val_mean_squared_error: 0.0215 - val_mean_absolute_error: 0.1184 - val_r2: -1664012.9981\n",
            "Epoch 58/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0230 - mean_squared_error: 0.0230 - mean_absolute_error: 0.1203 - r2: -853458.5068 - val_loss: 0.0279 - val_mean_squared_error: 0.0279 - val_mean_absolute_error: 0.1254 - val_r2: -2364195.3326\n",
            "Epoch 59/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0228 - mean_squared_error: 0.0228 - mean_absolute_error: 0.1186 - r2: -891179.7551 - val_loss: 0.0212 - val_mean_squared_error: 0.0212 - val_mean_absolute_error: 0.1178 - val_r2: -1623676.3616\n",
            "Epoch 60/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0221 - mean_squared_error: 0.0221 - mean_absolute_error: 0.1179 - r2: -712077.7699 - val_loss: 0.0215 - val_mean_squared_error: 0.0215 - val_mean_absolute_error: 0.1198 - val_r2: -1645234.9930\n",
            "Epoch 61/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0230 - mean_squared_error: 0.0230 - mean_absolute_error: 0.1200 - r2: -720182.5212 - val_loss: 0.0255 - val_mean_squared_error: 0.0255 - val_mean_absolute_error: 0.1284 - val_r2: -816766.1163\n",
            "Epoch 62/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0225 - mean_squared_error: 0.0225 - mean_absolute_error: 0.1182 - r2: -631653.4472 - val_loss: 0.0417 - val_mean_squared_error: 0.0417 - val_mean_absolute_error: 0.1420 - val_r2: -3363608.0041\n",
            "Epoch 63/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0236 - mean_squared_error: 0.0236 - mean_absolute_error: 0.1201 - r2: -702793.3127 - val_loss: 0.0473 - val_mean_squared_error: 0.0473 - val_mean_absolute_error: 0.1495 - val_r2: -3742406.3586\n",
            "Epoch 64/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0225 - mean_squared_error: 0.0225 - mean_absolute_error: 0.1195 - r2: -842630.2218 - val_loss: 0.0209 - val_mean_squared_error: 0.0209 - val_mean_absolute_error: 0.1176 - val_r2: -1530053.1135\n",
            "Epoch 65/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0226 - mean_squared_error: 0.0226 - mean_absolute_error: 0.1200 - r2: -598456.4924 - val_loss: 0.0572 - val_mean_squared_error: 0.0572 - val_mean_absolute_error: 0.1630 - val_r2: -4357139.7579\n",
            "Epoch 66/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0237 - mean_squared_error: 0.0237 - mean_absolute_error: 0.1217 - r2: -1053660.6264 - val_loss: 0.0241 - val_mean_squared_error: 0.0241 - val_mean_absolute_error: 0.1212 - val_r2: -1995508.1831\n",
            "Epoch 67/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0232 - mean_squared_error: 0.0232 - mean_absolute_error: 0.1194 - r2: -756881.8385 - val_loss: 0.0278 - val_mean_squared_error: 0.0278 - val_mean_absolute_error: 0.1337 - val_r2: -795110.1922\n",
            "Epoch 68/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0227 - mean_squared_error: 0.0227 - mean_absolute_error: 0.1202 - r2: -718895.4736 - val_loss: 0.0210 - val_mean_squared_error: 0.0210 - val_mean_absolute_error: 0.1214 - val_r2: -1479261.4050\n",
            "Epoch 69/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0228 - mean_squared_error: 0.0228 - mean_absolute_error: 0.1203 - r2: -727459.2218 - val_loss: 0.0201 - val_mean_squared_error: 0.0201 - val_mean_absolute_error: 0.1180 - val_r2: -1339225.8781\n",
            "Epoch 70/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0230 - mean_squared_error: 0.0230 - mean_absolute_error: 0.1201 - r2: -909271.9761 - val_loss: 0.0329 - val_mean_squared_error: 0.0329 - val_mean_absolute_error: 0.1312 - val_r2: -2764021.8934\n",
            "Epoch 71/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0237 - mean_squared_error: 0.0237 - mean_absolute_error: 0.1212 - r2: -809280.3469 - val_loss: 0.0208 - val_mean_squared_error: 0.0208 - val_mean_absolute_error: 0.1181 - val_r2: -1540037.6357\n",
            "Epoch 72/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0222 - mean_squared_error: 0.0222 - mean_absolute_error: 0.1181 - r2: -774095.6727 - val_loss: 0.0202 - val_mean_squared_error: 0.0202 - val_mean_absolute_error: 0.1177 - val_r2: -1369314.3246\n",
            "Epoch 73/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0225 - mean_squared_error: 0.0225 - mean_absolute_error: 0.1190 - r2: -1016146.7925 - val_loss: 0.0211 - val_mean_squared_error: 0.0211 - val_mean_absolute_error: 0.1180 - val_r2: -1591823.3493\n",
            "Epoch 74/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0223 - mean_squared_error: 0.0223 - mean_absolute_error: 0.1187 - r2: -710675.9160 - val_loss: 0.0203 - val_mean_squared_error: 0.0203 - val_mean_absolute_error: 0.1190 - val_r2: -1389148.9069\n",
            "Epoch 75/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0222 - mean_squared_error: 0.0222 - mean_absolute_error: 0.1184 - r2: -816792.3033 - val_loss: 0.0394 - val_mean_squared_error: 0.0394 - val_mean_absolute_error: 0.1400 - val_r2: -3266176.7315\n",
            "Epoch 76/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0233 - mean_squared_error: 0.0233 - mean_absolute_error: 0.1212 - r2: -983040.9066 - val_loss: 0.0311 - val_mean_squared_error: 0.0311 - val_mean_absolute_error: 0.1374 - val_r2: -717442.5430\n",
            "Epoch 77/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0235 - mean_squared_error: 0.0235 - mean_absolute_error: 0.1210 - r2: -870450.6179 - val_loss: 0.0210 - val_mean_squared_error: 0.0210 - val_mean_absolute_error: 0.1218 - val_r2: -1094120.0817\n",
            "Epoch 78/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0208 - mean_squared_error: 0.0208 - mean_absolute_error: 0.1165 - r2: -760783.1358 - val_loss: 0.0385 - val_mean_squared_error: 0.0385 - val_mean_absolute_error: 0.1377 - val_r2: -3060005.2679\n",
            "Epoch 79/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0225 - mean_squared_error: 0.0225 - mean_absolute_error: 0.1186 - r2: -773912.6394 - val_loss: 0.0229 - val_mean_squared_error: 0.0229 - val_mean_absolute_error: 0.1203 - val_r2: -1877844.1306\n",
            "Epoch 80/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0237 - mean_squared_error: 0.0237 - mean_absolute_error: 0.1211 - r2: -865983.2920 - val_loss: 0.0203 - val_mean_squared_error: 0.0203 - val_mean_absolute_error: 0.1195 - val_r2: -1347222.0545\n",
            "Epoch 81/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0227 - mean_squared_error: 0.0227 - mean_absolute_error: 0.1193 - r2: -576633.0561 - val_loss: 0.0203 - val_mean_squared_error: 0.0203 - val_mean_absolute_error: 0.1198 - val_r2: -1207634.7160\n",
            "Epoch 82/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0220 - mean_squared_error: 0.0220 - mean_absolute_error: 0.1181 - r2: -1021274.0979 - val_loss: 0.0232 - val_mean_squared_error: 0.0232 - val_mean_absolute_error: 0.1191 - val_r2: -1868378.2582\n",
            "Epoch 83/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0223 - mean_squared_error: 0.0223 - mean_absolute_error: 0.1177 - r2: -656717.9960 - val_loss: 0.0332 - val_mean_squared_error: 0.0332 - val_mean_absolute_error: 0.1407 - val_r2: -677821.7466\n",
            "Epoch 84/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0216 - mean_squared_error: 0.0216 - mean_absolute_error: 0.1182 - r2: -818330.0320 - val_loss: 0.0214 - val_mean_squared_error: 0.0214 - val_mean_absolute_error: 0.1174 - val_r2: -1622774.0011\n",
            "Epoch 85/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0224 - mean_squared_error: 0.0224 - mean_absolute_error: 0.1178 - r2: -572332.3528 - val_loss: 0.0207 - val_mean_squared_error: 0.0207 - val_mean_absolute_error: 0.1213 - val_r2: -1144489.9005\n",
            "Epoch 86/100\n",
            "131/131 [==============================] - 2s 13ms/step - loss: 0.0228 - mean_squared_error: 0.0228 - mean_absolute_error: 0.1202 - r2: -625207.7087 - val_loss: 0.0298 - val_mean_squared_error: 0.0298 - val_mean_absolute_error: 0.1358 - val_r2: -748962.0614\n",
            "Epoch 87/100\n",
            "131/131 [==============================] - 2s 13ms/step - loss: 0.0226 - mean_squared_error: 0.0226 - mean_absolute_error: 0.1193 - r2: -582999.7110 - val_loss: 0.0231 - val_mean_squared_error: 0.0231 - val_mean_absolute_error: 0.1252 - val_r2: -964041.9013\n",
            "Epoch 88/100\n",
            "131/131 [==============================] - 2s 13ms/step - loss: 0.0214 - mean_squared_error: 0.0214 - mean_absolute_error: 0.1180 - r2: -561393.1022 - val_loss: 0.0412 - val_mean_squared_error: 0.0412 - val_mean_absolute_error: 0.1412 - val_r2: -3277171.3745\n",
            "Epoch 89/100\n",
            "131/131 [==============================] - 2s 14ms/step - loss: 0.0224 - mean_squared_error: 0.0224 - mean_absolute_error: 0.1198 - r2: -665599.4691 - val_loss: 0.0207 - val_mean_squared_error: 0.0207 - val_mean_absolute_error: 0.1199 - val_r2: -1107658.4884\n",
            "Epoch 90/100\n",
            "131/131 [==============================] - 2s 15ms/step - loss: 0.0214 - mean_squared_error: 0.0214 - mean_absolute_error: 0.1167 - r2: -628830.5434 - val_loss: 0.0226 - val_mean_squared_error: 0.0226 - val_mean_absolute_error: 0.1193 - val_r2: -1808759.5809\n",
            "Epoch 91/100\n",
            "131/131 [==============================] - 2s 15ms/step - loss: 0.0236 - mean_squared_error: 0.0236 - mean_absolute_error: 0.1208 - r2: -901956.3289 - val_loss: 0.0203 - val_mean_squared_error: 0.0203 - val_mean_absolute_error: 0.1193 - val_r2: -1218799.2786\n",
            "Epoch 92/100\n",
            "131/131 [==============================] - 2s 13ms/step - loss: 0.0226 - mean_squared_error: 0.0226 - mean_absolute_error: 0.1202 - r2: -670669.4640 - val_loss: 0.0344 - val_mean_squared_error: 0.0344 - val_mean_absolute_error: 0.1326 - val_r2: -2773378.7006\n",
            "Epoch 93/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0230 - mean_squared_error: 0.0230 - mean_absolute_error: 0.1189 - r2: -624264.5559 - val_loss: 0.0211 - val_mean_squared_error: 0.0211 - val_mean_absolute_error: 0.1184 - val_r2: -1563861.6872\n",
            "Epoch 94/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0227 - mean_squared_error: 0.0227 - mean_absolute_error: 0.1196 - r2: -834237.9286 - val_loss: 0.0202 - val_mean_squared_error: 0.0202 - val_mean_absolute_error: 0.1179 - val_r2: -1253812.4189\n",
            "Epoch 95/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0216 - mean_squared_error: 0.0216 - mean_absolute_error: 0.1180 - r2: -622365.5064 - val_loss: 0.0202 - val_mean_squared_error: 0.0202 - val_mean_absolute_error: 0.1180 - val_r2: -1233048.3301\n",
            "Epoch 96/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0226 - mean_squared_error: 0.0226 - mean_absolute_error: 0.1196 - r2: -849806.4315 - val_loss: 0.0249 - val_mean_squared_error: 0.0249 - val_mean_absolute_error: 0.1205 - val_r2: -1966789.4003\n",
            "Epoch 97/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0224 - mean_squared_error: 0.0224 - mean_absolute_error: 0.1189 - r2: -553400.8215 - val_loss: 0.0209 - val_mean_squared_error: 0.0209 - val_mean_absolute_error: 0.1169 - val_r2: -1509074.3078\n",
            "Epoch 98/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0226 - mean_squared_error: 0.0226 - mean_absolute_error: 0.1194 - r2: -933918.0926 - val_loss: 0.0212 - val_mean_squared_error: 0.0212 - val_mean_absolute_error: 0.1218 - val_r2: -1097355.3232\n",
            "Epoch 99/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0225 - mean_squared_error: 0.0225 - mean_absolute_error: 0.1184 - r2: -736639.4696 - val_loss: 0.0202 - val_mean_squared_error: 0.0202 - val_mean_absolute_error: 0.1186 - val_r2: -1256950.2036\n",
            "Epoch 100/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0226 - mean_squared_error: 0.0226 - mean_absolute_error: 0.1197 - r2: -580771.1544 - val_loss: 0.0275 - val_mean_squared_error: 0.0275 - val_mean_absolute_error: 0.1322 - val_r2: -800482.0957\n",
            "Epoch 1/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0230 - mean_squared_error: 0.0230 - mean_absolute_error: 0.1201 - r2: -676342.0779 - val_loss: 0.0213 - val_mean_squared_error: 0.0213 - val_mean_absolute_error: 0.1172 - val_r2: -1571788.8914\n",
            "Epoch 2/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0235 - mean_squared_error: 0.0235 - mean_absolute_error: 0.1203 - r2: -687486.5762 - val_loss: 0.0204 - val_mean_squared_error: 0.0204 - val_mean_absolute_error: 0.1177 - val_r2: -1386446.2181\n",
            "Epoch 3/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0209 - mean_squared_error: 0.0209 - mean_absolute_error: 0.1162 - r2: -663341.4481 - val_loss: 0.0202 - val_mean_squared_error: 0.0202 - val_mean_absolute_error: 0.1184 - val_r2: -1283695.3467\n",
            "Epoch 4/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0217 - mean_squared_error: 0.0217 - mean_absolute_error: 0.1183 - r2: -794889.9092 - val_loss: 0.0216 - val_mean_squared_error: 0.0216 - val_mean_absolute_error: 0.1181 - val_r2: -1687545.9618\n",
            "Epoch 5/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0215 - mean_squared_error: 0.0215 - mean_absolute_error: 0.1175 - r2: -523706.1678 - val_loss: 0.0204 - val_mean_squared_error: 0.0204 - val_mean_absolute_error: 0.1176 - val_r2: -1248470.8364\n",
            "Epoch 6/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0223 - mean_squared_error: 0.0223 - mean_absolute_error: 0.1185 - r2: -537395.7451 - val_loss: 0.0260 - val_mean_squared_error: 0.0260 - val_mean_absolute_error: 0.1211 - val_r2: -2091051.1562\n",
            "Epoch 7/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0223 - mean_squared_error: 0.0223 - mean_absolute_error: 0.1182 - r2: -867949.1866 - val_loss: 0.0202 - val_mean_squared_error: 0.0202 - val_mean_absolute_error: 0.1188 - val_r2: -1283193.3459\n",
            "Epoch 8/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0224 - mean_squared_error: 0.0224 - mean_absolute_error: 0.1200 - r2: -510842.4652 - val_loss: 0.0202 - val_mean_squared_error: 0.0202 - val_mean_absolute_error: 0.1186 - val_r2: -1320191.9895\n",
            "Epoch 9/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0229 - mean_squared_error: 0.0229 - mean_absolute_error: 0.1196 - r2: -777676.4576 - val_loss: 0.0204 - val_mean_squared_error: 0.0204 - val_mean_absolute_error: 0.1198 - val_r2: -1199681.2778\n",
            "Epoch 10/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0225 - mean_squared_error: 0.0225 - mean_absolute_error: 0.1191 - r2: -840220.3563 - val_loss: 0.0211 - val_mean_squared_error: 0.0211 - val_mean_absolute_error: 0.1182 - val_r2: -1568265.4781\n",
            "Epoch 11/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0223 - mean_squared_error: 0.0223 - mean_absolute_error: 0.1199 - r2: -613384.1714 - val_loss: 0.0318 - val_mean_squared_error: 0.0318 - val_mean_absolute_error: 0.1298 - val_r2: -2685511.7075\n",
            "Epoch 12/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0229 - mean_squared_error: 0.0229 - mean_absolute_error: 0.1196 - r2: -1073300.1063 - val_loss: 0.0206 - val_mean_squared_error: 0.0206 - val_mean_absolute_error: 0.1207 - val_r2: -1167815.6608\n",
            "Epoch 13/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0222 - mean_squared_error: 0.0222 - mean_absolute_error: 0.1197 - r2: -763801.7150 - val_loss: 0.0267 - val_mean_squared_error: 0.0267 - val_mean_absolute_error: 0.1320 - val_r2: -814296.5520\n",
            "Epoch 14/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0219 - mean_squared_error: 0.0219 - mean_absolute_error: 0.1174 - r2: -592830.5468 - val_loss: 0.0207 - val_mean_squared_error: 0.0207 - val_mean_absolute_error: 0.1218 - val_r2: -1245961.1331\n",
            "Epoch 15/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0224 - mean_squared_error: 0.0224 - mean_absolute_error: 0.1197 - r2: -798010.0450 - val_loss: 0.0215 - val_mean_squared_error: 0.0215 - val_mean_absolute_error: 0.1185 - val_r2: -1653369.1052\n",
            "Epoch 16/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0223 - mean_squared_error: 0.0223 - mean_absolute_error: 0.1196 - r2: -524625.8383 - val_loss: 0.0233 - val_mean_squared_error: 0.0233 - val_mean_absolute_error: 0.1197 - val_r2: -1862522.6461\n",
            "Epoch 17/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0227 - mean_squared_error: 0.0227 - mean_absolute_error: 0.1200 - r2: -987679.8493 - val_loss: 0.0202 - val_mean_squared_error: 0.0202 - val_mean_absolute_error: 0.1191 - val_r2: -1272109.9147\n",
            "Epoch 18/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0228 - mean_squared_error: 0.0228 - mean_absolute_error: 0.1211 - r2: -685342.8864 - val_loss: 0.0204 - val_mean_squared_error: 0.0204 - val_mean_absolute_error: 0.1188 - val_r2: -1427997.4038\n",
            "Epoch 19/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0226 - mean_squared_error: 0.0226 - mean_absolute_error: 0.1196 - r2: -825003.0861 - val_loss: 0.0228 - val_mean_squared_error: 0.0228 - val_mean_absolute_error: 0.1179 - val_r2: -1728421.0920\n",
            "Epoch 20/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0227 - mean_squared_error: 0.0227 - mean_absolute_error: 0.1184 - r2: -589285.6068 - val_loss: 0.0204 - val_mean_squared_error: 0.0204 - val_mean_absolute_error: 0.1197 - val_r2: -1152432.1271\n",
            "Epoch 21/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0223 - mean_squared_error: 0.0223 - mean_absolute_error: 0.1195 - r2: -564362.8315 - val_loss: 0.0202 - val_mean_squared_error: 0.0202 - val_mean_absolute_error: 0.1183 - val_r2: -1295865.5991\n",
            "Epoch 22/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0221 - mean_squared_error: 0.0221 - mean_absolute_error: 0.1180 - r2: -881925.0866 - val_loss: 0.0347 - val_mean_squared_error: 0.0347 - val_mean_absolute_error: 0.1350 - val_r2: -2893284.4506\n",
            "Epoch 23/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0231 - mean_squared_error: 0.0231 - mean_absolute_error: 0.1217 - r2: -541877.2852 - val_loss: 0.0209 - val_mean_squared_error: 0.0209 - val_mean_absolute_error: 0.1207 - val_r2: -1126038.1702\n",
            "Epoch 24/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0229 - mean_squared_error: 0.0229 - mean_absolute_error: 0.1188 - r2: -625845.3701 - val_loss: 0.0277 - val_mean_squared_error: 0.0277 - val_mean_absolute_error: 0.1343 - val_r2: -811915.7563\n",
            "Epoch 25/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0227 - mean_squared_error: 0.0227 - mean_absolute_error: 0.1203 - r2: -562097.2899 - val_loss: 0.0244 - val_mean_squared_error: 0.0244 - val_mean_absolute_error: 0.1203 - val_r2: -1989198.3328\n",
            "Epoch 26/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0226 - mean_squared_error: 0.0226 - mean_absolute_error: 0.1200 - r2: -622089.8038 - val_loss: 0.0204 - val_mean_squared_error: 0.0204 - val_mean_absolute_error: 0.1200 - val_r2: -1246230.5727\n",
            "Epoch 27/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0216 - mean_squared_error: 0.0216 - mean_absolute_error: 0.1171 - r2: -749337.0827 - val_loss: 0.0205 - val_mean_squared_error: 0.0205 - val_mean_absolute_error: 0.1191 - val_r2: -1157369.3990\n",
            "Epoch 28/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0220 - mean_squared_error: 0.0220 - mean_absolute_error: 0.1175 - r2: -577481.6150 - val_loss: 0.0226 - val_mean_squared_error: 0.0226 - val_mean_absolute_error: 0.1218 - val_r2: -1781947.1502\n",
            "Epoch 29/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0236 - mean_squared_error: 0.0236 - mean_absolute_error: 0.1224 - r2: -864207.7544 - val_loss: 0.0204 - val_mean_squared_error: 0.0204 - val_mean_absolute_error: 0.1181 - val_r2: -1423797.1525\n",
            "Epoch 30/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0218 - mean_squared_error: 0.0218 - mean_absolute_error: 0.1171 - r2: -484114.9994 - val_loss: 0.0205 - val_mean_squared_error: 0.0205 - val_mean_absolute_error: 0.1184 - val_r2: -1470393.6955\n",
            "Epoch 31/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0228 - mean_squared_error: 0.0228 - mean_absolute_error: 0.1194 - r2: -672377.5439 - val_loss: 0.0265 - val_mean_squared_error: 0.0265 - val_mean_absolute_error: 0.1237 - val_r2: -2224715.4890\n",
            "Epoch 32/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0229 - mean_squared_error: 0.0229 - mean_absolute_error: 0.1196 - r2: -659043.4998 - val_loss: 0.0251 - val_mean_squared_error: 0.0251 - val_mean_absolute_error: 0.1211 - val_r2: -2070526.5391\n",
            "Epoch 33/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0239 - mean_squared_error: 0.0239 - mean_absolute_error: 0.1216 - r2: -1012541.0511 - val_loss: 0.0213 - val_mean_squared_error: 0.0213 - val_mean_absolute_error: 0.1186 - val_r2: -1616781.9015\n",
            "Epoch 34/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0226 - mean_squared_error: 0.0226 - mean_absolute_error: 0.1205 - r2: -954227.4574 - val_loss: 0.0348 - val_mean_squared_error: 0.0348 - val_mean_absolute_error: 0.1331 - val_r2: -2804894.6774\n",
            "Epoch 35/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0224 - mean_squared_error: 0.0224 - mean_absolute_error: 0.1196 - r2: -773244.4387 - val_loss: 0.0259 - val_mean_squared_error: 0.0259 - val_mean_absolute_error: 0.1303 - val_r2: -835201.7627\n",
            "Epoch 36/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0222 - mean_squared_error: 0.0222 - mean_absolute_error: 0.1197 - r2: -679598.7990 - val_loss: 0.0203 - val_mean_squared_error: 0.0203 - val_mean_absolute_error: 0.1189 - val_r2: -1328449.6026\n",
            "Epoch 37/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0229 - mean_squared_error: 0.0229 - mean_absolute_error: 0.1203 - r2: -691610.6264 - val_loss: 0.0238 - val_mean_squared_error: 0.0238 - val_mean_absolute_error: 0.1200 - val_r2: -1924672.5784\n",
            "Epoch 38/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0233 - mean_squared_error: 0.0233 - mean_absolute_error: 0.1213 - r2: -810502.1345 - val_loss: 0.0265 - val_mean_squared_error: 0.0265 - val_mean_absolute_error: 0.1237 - val_r2: -2194282.6537\n",
            "Epoch 39/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0227 - mean_squared_error: 0.0227 - mean_absolute_error: 0.1202 - r2: -801177.5372 - val_loss: 0.0240 - val_mean_squared_error: 0.0240 - val_mean_absolute_error: 0.1199 - val_r2: -1932859.4749\n",
            "Epoch 40/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0234 - mean_squared_error: 0.0234 - mean_absolute_error: 0.1207 - r2: -739302.4817 - val_loss: 0.0213 - val_mean_squared_error: 0.0213 - val_mean_absolute_error: 0.1225 - val_r2: -1082818.1081\n",
            "Epoch 41/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0229 - mean_squared_error: 0.0229 - mean_absolute_error: 0.1214 - r2: -788957.5876 - val_loss: 0.0205 - val_mean_squared_error: 0.0205 - val_mean_absolute_error: 0.1180 - val_r2: -1205692.1252\n",
            "Epoch 42/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0226 - mean_squared_error: 0.0226 - mean_absolute_error: 0.1191 - r2: -829357.9400 - val_loss: 0.0206 - val_mean_squared_error: 0.0206 - val_mean_absolute_error: 0.1198 - val_r2: -1416063.1815\n",
            "Epoch 43/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0231 - mean_squared_error: 0.0231 - mean_absolute_error: 0.1211 - r2: -909836.9073 - val_loss: 0.0229 - val_mean_squared_error: 0.0229 - val_mean_absolute_error: 0.1252 - val_r2: -972299.2634\n",
            "Epoch 44/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0226 - mean_squared_error: 0.0226 - mean_absolute_error: 0.1185 - r2: -710868.7972 - val_loss: 0.0204 - val_mean_squared_error: 0.0204 - val_mean_absolute_error: 0.1174 - val_r2: -1367934.4489\n",
            "Epoch 45/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0231 - mean_squared_error: 0.0231 - mean_absolute_error: 0.1211 - r2: -599346.7999 - val_loss: 0.0225 - val_mean_squared_error: 0.0225 - val_mean_absolute_error: 0.1187 - val_r2: -1765827.8414\n",
            "Epoch 46/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0225 - mean_squared_error: 0.0225 - mean_absolute_error: 0.1200 - r2: -815295.8125 - val_loss: 0.0248 - val_mean_squared_error: 0.0248 - val_mean_absolute_error: 0.1217 - val_r2: -2063646.3543\n",
            "Epoch 47/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0223 - mean_squared_error: 0.0223 - mean_absolute_error: 0.1193 - r2: -1074962.6215 - val_loss: 0.0378 - val_mean_squared_error: 0.0378 - val_mean_absolute_error: 0.1365 - val_r2: -3092301.7589\n",
            "Epoch 48/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0236 - mean_squared_error: 0.0236 - mean_absolute_error: 0.1217 - r2: -696197.4002 - val_loss: 0.0233 - val_mean_squared_error: 0.0233 - val_mean_absolute_error: 0.1201 - val_r2: -1862412.6008\n",
            "Epoch 49/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0233 - mean_squared_error: 0.0233 - mean_absolute_error: 0.1202 - r2: -938159.4330 - val_loss: 0.0211 - val_mean_squared_error: 0.0211 - val_mean_absolute_error: 0.1185 - val_r2: -1590831.5325\n",
            "Epoch 50/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0212 - mean_squared_error: 0.0212 - mean_absolute_error: 0.1157 - r2: -780121.0183 - val_loss: 0.0209 - val_mean_squared_error: 0.0209 - val_mean_absolute_error: 0.1217 - val_r2: -1133579.6761\n",
            "Epoch 51/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0229 - mean_squared_error: 0.0229 - mean_absolute_error: 0.1211 - r2: -584175.0160 - val_loss: 0.0203 - val_mean_squared_error: 0.0203 - val_mean_absolute_error: 0.1189 - val_r2: -1389457.2841\n",
            "Epoch 52/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0216 - mean_squared_error: 0.0216 - mean_absolute_error: 0.1169 - r2: -745867.2985 - val_loss: 0.0212 - val_mean_squared_error: 0.0212 - val_mean_absolute_error: 0.1180 - val_r2: -1602783.8908\n",
            "Epoch 53/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0216 - mean_squared_error: 0.0216 - mean_absolute_error: 0.1184 - r2: -637575.2588 - val_loss: 0.0325 - val_mean_squared_error: 0.0325 - val_mean_absolute_error: 0.1304 - val_r2: -2672717.6352\n",
            "Epoch 54/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0234 - mean_squared_error: 0.0234 - mean_absolute_error: 0.1197 - r2: -732921.1016 - val_loss: 0.0225 - val_mean_squared_error: 0.0225 - val_mean_absolute_error: 0.1194 - val_r2: -1779697.1192\n",
            "Epoch 55/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0236 - mean_squared_error: 0.0236 - mean_absolute_error: 0.1219 - r2: -741579.3759 - val_loss: 0.0203 - val_mean_squared_error: 0.0203 - val_mean_absolute_error: 0.1196 - val_r2: -1306106.9153\n",
            "Epoch 56/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0229 - mean_squared_error: 0.0229 - mean_absolute_error: 0.1204 - r2: -736347.9812 - val_loss: 0.0211 - val_mean_squared_error: 0.0211 - val_mean_absolute_error: 0.1177 - val_r2: -1600726.0899\n",
            "Epoch 57/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0232 - mean_squared_error: 0.0232 - mean_absolute_error: 0.1197 - r2: -935793.1849 - val_loss: 0.0232 - val_mean_squared_error: 0.0232 - val_mean_absolute_error: 0.1203 - val_r2: -1887513.8406\n",
            "Epoch 58/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0229 - mean_squared_error: 0.0229 - mean_absolute_error: 0.1206 - r2: -695103.5133 - val_loss: 0.0210 - val_mean_squared_error: 0.0210 - val_mean_absolute_error: 0.1173 - val_r2: -1548637.6195\n",
            "Epoch 59/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0231 - mean_squared_error: 0.0231 - mean_absolute_error: 0.1184 - r2: -901095.3699 - val_loss: 0.0204 - val_mean_squared_error: 0.0204 - val_mean_absolute_error: 0.1188 - val_r2: -1388018.4077\n",
            "Epoch 60/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0237 - mean_squared_error: 0.0237 - mean_absolute_error: 0.1218 - r2: -841442.9946 - val_loss: 0.0215 - val_mean_squared_error: 0.0215 - val_mean_absolute_error: 0.1193 - val_r2: -1671096.6710\n",
            "Epoch 61/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0232 - mean_squared_error: 0.0232 - mean_absolute_error: 0.1192 - r2: -555634.5770 - val_loss: 0.0310 - val_mean_squared_error: 0.0310 - val_mean_absolute_error: 0.1294 - val_r2: -2625486.2929\n",
            "Epoch 62/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0231 - mean_squared_error: 0.0231 - mean_absolute_error: 0.1202 - r2: -1160826.5863 - val_loss: 0.0226 - val_mean_squared_error: 0.0226 - val_mean_absolute_error: 0.1198 - val_r2: -1807159.8842\n",
            "Epoch 63/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0231 - mean_squared_error: 0.0231 - mean_absolute_error: 0.1214 - r2: -903517.8570 - val_loss: 0.0248 - val_mean_squared_error: 0.0248 - val_mean_absolute_error: 0.1273 - val_r2: -877191.0541\n",
            "Epoch 64/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0239 - mean_squared_error: 0.0239 - mean_absolute_error: 0.1220 - r2: -607112.5977 - val_loss: 0.0210 - val_mean_squared_error: 0.0210 - val_mean_absolute_error: 0.1198 - val_r2: -1550242.6374\n",
            "Epoch 65/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0234 - mean_squared_error: 0.0234 - mean_absolute_error: 0.1214 - r2: -761137.9736 - val_loss: 0.0223 - val_mean_squared_error: 0.0223 - val_mean_absolute_error: 0.1187 - val_r2: -1761905.9603\n",
            "Epoch 66/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0230 - mean_squared_error: 0.0230 - mean_absolute_error: 0.1199 - r2: -864926.8992 - val_loss: 0.0292 - val_mean_squared_error: 0.0292 - val_mean_absolute_error: 0.1263 - val_r2: -2384997.7253\n",
            "Epoch 67/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0215 - mean_squared_error: 0.0215 - mean_absolute_error: 0.1174 - r2: -643544.9451 - val_loss: 0.0275 - val_mean_squared_error: 0.0275 - val_mean_absolute_error: 0.1324 - val_r2: -804028.1332\n",
            "Epoch 68/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0229 - mean_squared_error: 0.0229 - mean_absolute_error: 0.1205 - r2: -860068.3388 - val_loss: 0.0219 - val_mean_squared_error: 0.0219 - val_mean_absolute_error: 0.1192 - val_r2: -1736751.4670\n",
            "Epoch 69/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0221 - mean_squared_error: 0.0221 - mean_absolute_error: 0.1178 - r2: -869873.3671 - val_loss: 0.0207 - val_mean_squared_error: 0.0207 - val_mean_absolute_error: 0.1183 - val_r2: -1498533.7909\n",
            "Epoch 70/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0235 - mean_squared_error: 0.0235 - mean_absolute_error: 0.1210 - r2: -919375.7456 - val_loss: 0.0213 - val_mean_squared_error: 0.0213 - val_mean_absolute_error: 0.1184 - val_r2: -1610142.7627\n",
            "Epoch 71/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0226 - mean_squared_error: 0.0226 - mean_absolute_error: 0.1203 - r2: -637406.6576 - val_loss: 0.0203 - val_mean_squared_error: 0.0203 - val_mean_absolute_error: 0.1192 - val_r2: -1282744.3918\n",
            "Epoch 72/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0213 - mean_squared_error: 0.0213 - mean_absolute_error: 0.1166 - r2: -584912.6534 - val_loss: 0.0379 - val_mean_squared_error: 0.0379 - val_mean_absolute_error: 0.1488 - val_r2: -655214.9250\n",
            "Epoch 73/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0233 - mean_squared_error: 0.0233 - mean_absolute_error: 0.1199 - r2: -639132.9629 - val_loss: 0.0205 - val_mean_squared_error: 0.0205 - val_mean_absolute_error: 0.1201 - val_r2: -1378374.6456\n",
            "Epoch 74/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0223 - mean_squared_error: 0.0223 - mean_absolute_error: 0.1170 - r2: -740593.1660 - val_loss: 0.0231 - val_mean_squared_error: 0.0231 - val_mean_absolute_error: 0.1202 - val_r2: -1879297.4068\n",
            "Epoch 75/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0224 - mean_squared_error: 0.0224 - mean_absolute_error: 0.1199 - r2: -766571.1868 - val_loss: 0.0232 - val_mean_squared_error: 0.0232 - val_mean_absolute_error: 0.1249 - val_r2: -951513.7289\n",
            "Epoch 76/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0225 - mean_squared_error: 0.0225 - mean_absolute_error: 0.1198 - r2: -766221.4851 - val_loss: 0.0221 - val_mean_squared_error: 0.0221 - val_mean_absolute_error: 0.1188 - val_r2: -1746917.9754\n",
            "Epoch 77/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0226 - mean_squared_error: 0.0226 - mean_absolute_error: 0.1196 - r2: -825163.9068 - val_loss: 0.0209 - val_mean_squared_error: 0.0209 - val_mean_absolute_error: 0.1221 - val_r2: -1184924.1831\n",
            "Epoch 78/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0233 - mean_squared_error: 0.0233 - mean_absolute_error: 0.1211 - r2: -837984.1701 - val_loss: 0.0204 - val_mean_squared_error: 0.0204 - val_mean_absolute_error: 0.1199 - val_r2: -1279671.2692\n",
            "Epoch 79/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0224 - mean_squared_error: 0.0224 - mean_absolute_error: 0.1191 - r2: -630181.7986 - val_loss: 0.0256 - val_mean_squared_error: 0.0256 - val_mean_absolute_error: 0.1229 - val_r2: -2164299.3479\n",
            "Epoch 80/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0217 - mean_squared_error: 0.0217 - mean_absolute_error: 0.1177 - r2: -743686.0419 - val_loss: 0.0203 - val_mean_squared_error: 0.0203 - val_mean_absolute_error: 0.1188 - val_r2: -1307049.6453\n",
            "Epoch 81/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0230 - mean_squared_error: 0.0230 - mean_absolute_error: 0.1195 - r2: -792345.8344 - val_loss: 0.0289 - val_mean_squared_error: 0.0289 - val_mean_absolute_error: 0.1271 - val_r2: -2398936.1824\n",
            "Epoch 82/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0236 - mean_squared_error: 0.0236 - mean_absolute_error: 0.1221 - r2: -865369.1564 - val_loss: 0.0227 - val_mean_squared_error: 0.0227 - val_mean_absolute_error: 0.1198 - val_r2: -1798727.0604\n",
            "Epoch 83/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0235 - mean_squared_error: 0.0235 - mean_absolute_error: 0.1206 - r2: -729916.4904 - val_loss: 0.0203 - val_mean_squared_error: 0.0203 - val_mean_absolute_error: 0.1192 - val_r2: -1361225.4723\n",
            "Epoch 84/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0236 - mean_squared_error: 0.0236 - mean_absolute_error: 0.1218 - r2: -758028.5203 - val_loss: 0.0212 - val_mean_squared_error: 0.0212 - val_mean_absolute_error: 0.1220 - val_r2: -1098295.8844\n",
            "Epoch 85/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0224 - mean_squared_error: 0.0224 - mean_absolute_error: 0.1190 - r2: -632589.6140 - val_loss: 0.0222 - val_mean_squared_error: 0.0222 - val_mean_absolute_error: 0.1192 - val_r2: -1778774.3444\n",
            "Epoch 86/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0228 - mean_squared_error: 0.0228 - mean_absolute_error: 0.1185 - r2: -814399.3856 - val_loss: 0.0203 - val_mean_squared_error: 0.0203 - val_mean_absolute_error: 0.1197 - val_r2: -1298703.5809\n",
            "Epoch 87/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0227 - mean_squared_error: 0.0227 - mean_absolute_error: 0.1209 - r2: -685841.3611 - val_loss: 0.0230 - val_mean_squared_error: 0.0230 - val_mean_absolute_error: 0.1191 - val_r2: -1869013.1045\n",
            "Epoch 88/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0230 - mean_squared_error: 0.0230 - mean_absolute_error: 0.1208 - r2: -1066002.2365 - val_loss: 0.0203 - val_mean_squared_error: 0.0203 - val_mean_absolute_error: 0.1182 - val_r2: -1281406.8468\n",
            "Epoch 89/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0229 - mean_squared_error: 0.0229 - mean_absolute_error: 0.1188 - r2: -833837.7114 - val_loss: 0.0296 - val_mean_squared_error: 0.0296 - val_mean_absolute_error: 0.1261 - val_r2: -2391645.0912\n",
            "Epoch 90/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0220 - mean_squared_error: 0.0220 - mean_absolute_error: 0.1185 - r2: -671418.1202 - val_loss: 0.0240 - val_mean_squared_error: 0.0240 - val_mean_absolute_error: 0.1211 - val_r2: -1983019.1873\n",
            "Epoch 91/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0236 - mean_squared_error: 0.0236 - mean_absolute_error: 0.1222 - r2: -825743.8999 - val_loss: 0.0203 - val_mean_squared_error: 0.0203 - val_mean_absolute_error: 0.1194 - val_r2: -1325990.2635\n",
            "Epoch 92/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0232 - mean_squared_error: 0.0232 - mean_absolute_error: 0.1202 - r2: -712206.7350 - val_loss: 0.0205 - val_mean_squared_error: 0.0205 - val_mean_absolute_error: 0.1187 - val_r2: -1452533.0309\n",
            "Epoch 93/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0212 - mean_squared_error: 0.0212 - mean_absolute_error: 0.1170 - r2: -684864.1892 - val_loss: 0.0213 - val_mean_squared_error: 0.0213 - val_mean_absolute_error: 0.1175 - val_r2: -1609337.9737\n",
            "Epoch 94/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0229 - mean_squared_error: 0.0229 - mean_absolute_error: 0.1196 - r2: -640076.0066 - val_loss: 0.0221 - val_mean_squared_error: 0.0221 - val_mean_absolute_error: 0.1191 - val_r2: -1753181.7035\n",
            "Epoch 95/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0229 - mean_squared_error: 0.0229 - mean_absolute_error: 0.1210 - r2: -680341.0916 - val_loss: 0.0204 - val_mean_squared_error: 0.0204 - val_mean_absolute_error: 0.1185 - val_r2: -1426227.5383\n",
            "Epoch 96/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0234 - mean_squared_error: 0.0234 - mean_absolute_error: 0.1211 - r2: -598248.4707 - val_loss: 0.0204 - val_mean_squared_error: 0.0204 - val_mean_absolute_error: 0.1200 - val_r2: -1225665.3000\n",
            "Epoch 97/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0234 - mean_squared_error: 0.0234 - mean_absolute_error: 0.1211 - r2: -763838.9165 - val_loss: 0.0217 - val_mean_squared_error: 0.0217 - val_mean_absolute_error: 0.1232 - val_r2: -1076050.6808\n",
            "Epoch 98/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0215 - mean_squared_error: 0.0215 - mean_absolute_error: 0.1181 - r2: -590718.2904 - val_loss: 0.0246 - val_mean_squared_error: 0.0246 - val_mean_absolute_error: 0.1209 - val_r2: -2061514.5244\n",
            "Epoch 99/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0227 - mean_squared_error: 0.0227 - mean_absolute_error: 0.1191 - r2: -967731.4801 - val_loss: 0.0203 - val_mean_squared_error: 0.0203 - val_mean_absolute_error: 0.1191 - val_r2: -1389720.0271\n",
            "Epoch 100/100\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 0.0228 - mean_squared_error: 0.0228 - mean_absolute_error: 0.1204 - r2: -738611.5300 - val_loss: 0.0204 - val_mean_squared_error: 0.0204 - val_mean_absolute_error: 0.1200 - val_r2: -1239399.0168\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XGYfo3-eP8L8",
        "colab_type": "code",
        "outputId": "8e3d0101-ee88-4d77-c27f-f2805d7e6af5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "cell_type": "code",
      "source": [
        "loss, mse, mae, r2_value = model.evaluate(test_input, test_output)\n",
        "\n",
        "print(\"MSE: \", mse)\n",
        "print(\"MAE: \", mae)\n",
        "print(\"R2:  \", r2_value)\n",
        "print(\"Loss:\", loss)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "30/30 [==============================] - 0s 5ms/step\n",
            "MSE:  0.02283925020487419\n",
            "MAE:  0.12278956320208273\n",
            "R2:   -2556737.8590896954\n",
            "Loss: 0.02283925020487419\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "j14-1yqxOwld",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4265c98d-a468-4573-f3d9-8065428b1426"
      },
      "cell_type": "code",
      "source": [
        "ttf = model.predict(data, verbose = 1)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "220/220 [==============================] - 0s 2ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4CkMwbIjPZk6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ttf\n",
        "df_ttf = pd.DataFrame(ttf)\n",
        "df_ttf.to_csv('ttf.csv')\n",
        "#!cat ttf.csv\n",
        "#from google.colab import files\n",
        "#files.download('ttf.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WoMtvL5pUBtE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d02b61cc-2496-46fc-ed53-c859faff8554"
      },
      "cell_type": "code",
      "source": [
        "uploaded = drive.CreateFile({'title': 'ttf2.csv'})\n",
        "uploaded.SetContentString(df_ttf.to_string())\n",
        "uploaded.Upload()\n",
        "print('Uploaded file with ID {}'.format(uploaded.get('id')))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uploaded file with ID 1bhWWl2je7ryLjbTbcWrFmjoknuyoTuIc\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "V3V6ouyo1Xfu",
        "colab_type": "code",
        "outputId": "f77d363b-ea92-40a8-bf55-30166a0ea018",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        }
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot training & validation accuracy values\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAFnCAYAAAC/5tBZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f7d92aaa9b0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "Q-kdVmyVYieY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Conclusion"
      ]
    },
    {
      "metadata": {
        "id": "vfOWiI-lDak0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The model passed proof of concept phase, it does learn and gives OK prediction, more data will lead to even more accurate results."
      ]
    }
  ]
}